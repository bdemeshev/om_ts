% !TEX root = ../om_ts_07.tex

\begin{frame} % название фрагмента

\videotitle{Как обойтись без моделей?}

\end{frame}



\begin{frame}{Обойтись без моделей: план}
  \begin{itemize}[<+->]
    \item \alert{Лаги зависимой} переменной. 
    \item \alert{Агрегирующие функции}.
    \item \alert{Скользящее и растущее окно}. 
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Как обойтись без моделей?}

  \begin{block}{Старые друзья}
    Есть алгоритмы, которые по обучающей выборке зависимой переменной $y$, 
    обучающей матрице предикторов $X$, и новым предикторам $X_F$ строят прогноз $\hat y_F$.      
  \end{block}

  \pause

  \alert{Случайный лес}, \alert{градиентный бустинг}\ldots \pause и даже \alert{простая регрессия}!

  \pause 

  Можно \alert{усреднять} прогнозы ARIMA/ETS и прогнозы других алгоритмов!

\end{frame}


\begin{frame}
  \frametitle{Как создать предикторы?}

  Как из одного столбца $y$ создать целую матрицу $X$ предикторов?

  \begin{itemize}[<+->]
    \item Использовать лаги $y_{t-k}$. 
    \item Использовать функции от лагов в качестве предикторов. 
  \end{itemize}


\end{frame}

\begin{frame}
  \frametitle{Используем лаги $y$}

  Для примера возьмём два лага, $Ly_t$ и $L^2 y_t$.
  \pause

  Обучающая выборка:
  \[
  \begin{pmatrix}
    y_3 \\
    y_4 \\
    y_5 \\
    \vdots \\
    y_T 
  \end{pmatrix}  \quad 
  \begin{pmatrix}
    y_1 & y_2 \\
    y_2 & y_3 \\
    y_3 & y_4 \\
    \vdots \\
    y_{T-2} & y_{T-1} \\ 
  \end{pmatrix}
  \]
  \pause
  Выборка для прогнозирования:
  \[
  \begin{pmatrix}
    ? 
  \end{pmatrix}  \quad 
  \begin{pmatrix}
    y_{T-1} & y_{T} \\ 
  \end{pmatrix}
  \]
  
\end{frame}

\begin{frame}
  \frametitle{Сколько лагов добавить?}

  \begin{itemize}[<+->]
    \item Каждый добавленный лаг \alert{сокращает} обучающую выборку!
    \item Разумно добавить \alert{ближайшие лаги} $Ly_t$, $L^2y_t$.
    \item Для сезонных данных разумно добавить \alert{сезонный лаг} $L^{12} y_t$.
    \item Есть алгоритмы \alert{чувствительные к лишним предикторам}: например, регрессия. 
    \item Есть алгоритмы \alert{нечувствительные к лишним предикторам}: например, регрессия.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Функции от лагов}
  При прогнозировании $y_{t}$ \alert{честно} использовать любую функцию от \alert{предыдущих} $y_{t-1}$, $y_{t-2}$, \ldots

  \pause

  Например:
  \begin{itemize}[<+->]
    \item $\Delta y_{t-1} = y_{t-1} - y_{t-2}$;
    \item $\max\{ y_{t-1}, y_{t-2}, y_{t-3} \}$;
    \item $\min\{ y_{t-1}, y_{t-2}, \ldots, y_1\}$;
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Типичный предиктор}

  \begin{itemize}[<+->]
    \item \alert{Агрегирующая функция}:
    
    Минимум, максимум, среднее, медиана, размах, выборочная дисперсия, выборочное стандартное отклонение, \ldots

    \item \alert{Аргумент} агрегирующей функции:
    
    \alert{Скользящее окно}: агрегирующая функция применяется, скажем, к трём предыдущим значениям $y_{t-1}$, $y_{t-2}$, $y_{t-3}$.

    \alert{Растущее окно}: агрегирующая функция применяется ко всем предыдущим значениям $y_{t-1}$, $y_{t-2}$, \ldots,  $y_{1}$.
  \end{itemize}

\end{frame}



\begin{frame}{Обойтись без моделей: итоги}

  \begin{itemize}[<+->]
    \item Помните о \alert{случайном лесе}, \alert{градиентном бустинге} и даже о \alert{простой регрессии}.
    \item Добавьте \alert{лаги зависимой переменной}.
    \item Добавьте \alert{агрегирующие функции} скользящим и растущим окном. 
  \end{itemize}
\end{frame}

